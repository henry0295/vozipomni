# =============================================================================
# VoziPOmni Contact Center — Docker Compose (Desarrollo Local)
# =============================================================================
# Uso:
#   cp env.template .env          # Editar variables
#   docker compose up -d          # Iniciar servicios
#   docker compose logs -f        # Ver logs
# =============================================================================

# ─── YAML Templates (Anchors) ────────────────────────────────────────────────
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

x-restart-policy: &restart-policy
  restart: unless-stopped

x-healthcheck-http: &healthcheck-http
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 60s

x-healthcheck-tcp: &healthcheck-tcp
  interval: 15s
  timeout: 5s
  retries: 5
  start_period: 30s

# ─── Django common env ──────────────────────────────────────────────────────
x-django-env: &django-env
  DEBUG: "True"
  SECRET_KEY: ${SECRET_KEY:-dev-secret-key-change-in-production}
  ALLOWED_HOSTS: ${ALLOWED_HOSTS:-*}
  CORS_ORIGINS: ${CORS_ORIGINS:-http://localhost:3000,http://localhost}
  DB_NAME: ${POSTGRES_DB:-vozipomni}
  DB_USER: ${POSTGRES_USER:-vozipomni_user}
  DB_PASSWORD: ${POSTGRES_PASSWORD:-vozipomni_db_2026}
  DB_HOST: postgres
  DB_PORT: "5432"
  REDIS_URL: redis://:${REDIS_PASSWORD:-vozipomni_redis_2026}@redis:6379/0
  CELERY_BROKER_URL: redis://:${REDIS_PASSWORD:-vozipomni_redis_2026}@redis:6379/0
  CELERY_RESULT_BACKEND: redis://:${REDIS_PASSWORD:-vozipomni_redis_2026}@redis:6379/1
  ASTERISK_HOST: ${VOZIPOMNI_IPV4:-asterisk}
  ASTERISK_AMI_PORT: "5038"
  ASTERISK_AMI_USER: ${ASTERISK_AMI_USER:-admin}
  ASTERISK_AMI_PASSWORD: ${ASTERISK_AMI_PASSWORD:-vozipomni_ami_2026}
  ASTERISK_CONFIG_DIR: /var/lib/asterisk/dynamic
  ASTERISK_PUBLIC_IP: ${VOZIPOMNI_IPV4:-127.0.0.1}
  TZ: ${TZ:-America/Bogota}

# ─── Django common service ──────────────────────────────────────────────────
x-django-common: &django-common
  build:
    context: ./backend
    dockerfile: Dockerfile
  dns: 8.8.8.8
  networks:
    - vozipomni_network
  <<: *restart-policy
  logging: *default-logging
  stdin_open: true
  tty: true

# =============================================================================
# SERVICES
# =============================================================================
services:

  # ──────────────────────────────────────────────────────────────────────────
  # DATA STORES (PostgreSQL, Redis)
  # ──────────────────────────────────────────────────────────────────────────
  postgres:
    image: postgres:14-alpine
    container_name: vozipomni_postgres
    dns: 8.8.8.8
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-vozipomni}
      POSTGRES_USER: ${POSTGRES_USER:-vozipomni_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-vozipomni_db_2026}
      TZ: ${TZ:-America/Bogota}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgresql/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "${VOZIPOMNI_IPV4:-0.0.0.0}:5432:5432/tcp"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-vozipomni_user} -d ${POSTGRES_DB:-vozipomni} -q"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 20s
    <<: *restart-policy
    logging: *default-logging
    stop_grace_period: 90s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    networks:
      - vozipomni_network

  redis:
    image: redis:7-alpine
    container_name: vozipomni_redis
    dns: 8.8.8.8
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-vozipomni_redis_2026}
    environment:
      TZ: ${TZ:-America/Bogota}
    volumes:
      - redis_data:/data
    ports:
      - "${VOZIPOMNI_IPV4:-0.0.0.0}:6379:6379/tcp"
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-vozipomni_redis_2026}", "ping"]
      <<: *healthcheck-tcp
    <<: *restart-policy
    logging: *default-logging
    stop_grace_period: 30s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.05'
    networks:
      - vozipomni_network

  # ──────────────────────────────────────────────────────────────────────────
  # DJANGO APPLICATION SERVICES
  # ──────────────────────────────────────────────────────────────────────────
  backend:
    <<: *django-common
    container_name: vozipomni_backend
    command: >
      sh -c "python manage.py migrate --noinput &&
             python manage.py collectstatic --noinput &&
             gunicorn config.wsgi:application
               --bind 0.0.0.0:8000
               --workers ${GUNICORN_WORKERS:-4}
               --timeout ${GUNICORN_TIMEOUT:-120}
               --access-logfile -
               --error-logfile -"
    environment:
      <<: *django-env
    volumes:
      - ./backend:/app
      - static_volume:/app/static
      - media_volume:/app/media
      - recordings_volume:/app/recordings
      - ./docker/asterisk/configs:/etc/asterisk
      - asterisk_dynamic:/var/lib/asterisk/dynamic
      - asterisk_recordings:/var/spool/asterisk/monitor
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -fsSL -o /dev/null http://localhost:8000/api/ || exit 1"]
      <<: *healthcheck-http
      start_period: 90s
    stop_grace_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.25'

  celery_worker:
    <<: *django-common
    container_name: vozipomni_celery_worker
    command: celery -A config worker --loglevel=${CELERY_LOG_LEVEL:-info} --concurrency=${CELERY_CONCURRENCY:-4}
    environment:
      <<: *django-env
    volumes:
      - ./backend:/app
      - recordings_volume:/app/recordings
      - ./docker/asterisk/configs:/etc/asterisk
      - asterisk_dynamic:/var/lib/asterisk/dynamic
      - asterisk_recordings:/var/spool/asterisk/monitor
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      backend:
        condition: service_healthy
    stop_grace_period: 30s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M

  celery_beat:
    <<: *django-common
    container_name: vozipomni_celery_beat
    command: celery -A config beat --loglevel=${CELERY_LOG_LEVEL:-info} --scheduler django_celery_beat.schedulers:DatabaseScheduler
    environment:
      <<: *django-env
    volumes:
      - ./backend:/app
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      backend:
        condition: service_healthy
    stop_grace_period: 30s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 64M

  # ──────────────────────────────────────────────────────────────────────────
  # TELEPHONY (Asterisk)
  # ──────────────────────────────────────────────────────────────────────────
  asterisk:
    build:
      context: ./docker/asterisk
      dockerfile: Dockerfile
    container_name: vozipomni_asterisk
    hostname: asterisk
    environment:
      VOZIPOMNI_IPV4: ${VOZIPOMNI_IPV4:-}
      NAT_IPV4: ${NAT_IPV4:-}
      TZ: ${TZ:-America/Bogota}
    volumes:
      - ./docker/asterisk/configs:/etc/asterisk
      - asterisk_recordings:/var/spool/asterisk/monitor
      - asterisk_sounds:/var/lib/asterisk/sounds
      - asterisk_dynamic:/var/lib/asterisk/dynamic
    healthcheck:
      test: ["CMD-SHELL", "asterisk -rx 'core show channels' | grep -q 'active channels' || exit 1"]
      <<: *healthcheck-tcp
      start_period: 60s
    privileged: true
    <<: *restart-policy
    logging: *default-logging
    stop_grace_period: 30s
    # En desarrollo usamos bridge con puertos mapeados (en producción network_mode: host)
    ports:
      - "5060:5060/udp"
      - "5060:5060/tcp"
      - "5061:5061/tcp"
      - "5161:5161/udp"
      - "5162:5162/udp"
      - "5038:5038"
      - "8088:8088"
      - "8089:8089"
      - "10000-10100:10000-10100/udp"
    networks:
      - vozipomni_network

  # ──────────────────────────────────────────────────────────────────────────
  # WEB & PROXY SERVICES
  # ──────────────────────────────────────────────────────────────────────────
  nginx:
    image: nginx:alpine
    container_name: vozipomni_nginx
    dns: 8.8.8.8
    environment:
      TZ: ${TZ:-America/Bogota}
    volumes:
      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./docker/nginx/default.conf:/etc/nginx/conf.d/default.conf:ro
      - static_volume:/app/static:ro
      - media_volume:/app/media:ro
    ports:
      - "${NGINX_HTTP_PORT:-80}:80"
      - "${NGINX_HTTPS_PORT:-443}:443"
    depends_on:
      backend:
        condition: service_healthy
      frontend:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "curl -fsSL -o /dev/null http://localhost/ || exit 1"]
      <<: *healthcheck-http
      start_period: 30s
    <<: *restart-policy
    logging: *default-logging
    stop_grace_period: 15s
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 64M
    networks:
      - vozipomni_network

  # ──────────────────────────────────────────────────────────────────────────
  # FRONTEND
  # ──────────────────────────────────────────────────────────────────────────
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: vozipomni_frontend
    dns: 8.8.8.8
    environment:
      NODE_ENV: production
      NUXT_HOST: "0.0.0.0"
      NUXT_PORT: "3000"
      NUXT_PUBLIC_API_BASE: ${NUXT_PUBLIC_API_BASE:-/api}
      NUXT_PUBLIC_WS_BASE: ${NUXT_PUBLIC_WS_BASE:-/ws}
      NUXT_PUBLIC_APP_NAME: ${NUXT_PUBLIC_APP_NAME:-VozipOmni}
      TZ: ${TZ:-America/Bogota}
    depends_on:
      backend:
        condition: service_healthy
    <<: *restart-policy
    logging: *default-logging
    stop_grace_period: 15s
    ports:
      - "3000:3000"
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M
    networks:
      - vozipomni_network

  # ──────────────────────────────────────────────────────────────────────────
  # FRONTEND DEV (solo con profile "dev")
  # ──────────────────────────────────────────────────────────────────────────
  frontend_dev:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: vozipomni_frontend_dev
    volumes:
      - ./frontend:/app
      - /app/node_modules
      - /app/.nuxt
    ports:
      - "3001:3000"
    environment:
      NODE_ENV: development
      NUXT_PUBLIC_API_BASE: ${NUXT_PUBLIC_API_BASE:-/api}
      NUXT_PUBLIC_WS_BASE: ${NUXT_PUBLIC_WS_BASE:-/ws}
    networks:
      - vozipomni_network
    stdin_open: true
    tty: true
    profiles:
      - dev
    logging: *default-logging

# =============================================================================
# VOLUMES
# =============================================================================
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  static_volume:
    driver: local
  media_volume:
    driver: local
  recordings_volume:
    driver: local
  asterisk_recordings:
    driver: local
  asterisk_sounds:
    driver: local
  asterisk_dynamic:
    driver: local

# =============================================================================
# NETWORKS
# =============================================================================
networks:
  vozipomni_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: ${SUBNET:-172.25.0.0/16}
